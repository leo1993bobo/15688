{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0n_E6xJSpFZ"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "This tutorial will introduce you to a new method of learning --- Ensemble learning, particularly focusing on algorithms behind Boosting and Bagging. The idea of ensemble learning is to build a predictive model by combining a collection of simpler base models, also known as weak learners or base learners.\n",
    "It is well known that an ensemble learning is usually more accurate than a single learner, and ensemble methods have already achieved great success in many real-world problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvVT4GSRIEN9"
   },
   "source": [
    "## Tutorial content\n",
    "\n",
    "In this tutorial, we will show two types of ensemble learning: Bagging and Boosting. And discuss in depth about popular model in each type: Random Forest and Adaboost, including the algorithms behind them and how to use them in Python. This tutorial is designed to focus on the algorithm side and conceptional side. The coding part is just a good support of a better understanding. This tutorial also includes a practice question about the iteration of Adaboost to deepen your knowledge. Are you ready to start the journey?\n",
    "\n",
    "**Content**:\n",
    "* <a href = '#Bagging'>Bagging</a>\n",
    "    * <a href = '#General'>General</a>\n",
    "    * <a href = '#Coding'>Coding</a>\n",
    "    * <a href = '#Random_Forest'>Random Forest</a>\n",
    "    * <a href = '#Algorithm_of_Random_Forest'>Algorithm of Random Forest</a>\n",
    "* <a href = '#Boosting'>Boosting</a>\n",
    "    * <a href = '#Regression'>Regression</a>\n",
    "    * <a href = '#Classification'>Classification</a>\n",
    "    * <a href = '#Algorithm_of_Adaboost'>Algorithm of Adaboost</a>\n",
    "    * <a href = '#Practice'>Practice</a>\n",
    "    * <a href = '#Coding'>Coding</a>\n",
    "* <a href = '#Bagging_vs_Boosting'>Bagging_vs_Boosting</a>\n",
    "* <a href = '#References'>References</a>\n",
    "* <a href = '#Useful_Resources'>Useful_Resources</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzGjJuRFDULP"
   },
   "source": [
    "# **Bagging**\n",
    "## General\n",
    "The name \"Bagging\" came from the abbreviation of Bootstrap Aggregation [Breiman, 1996]. As we could tell from the name itself, the two key elements of this method are bootstrap and aggregation. It is a general-purpose method to reduce the variance of a learning method by averaging together multiple estimates. \n",
    "\n",
    "It applies bootstrapping to gain data subsets for training the base learners. To be specific, it takes repeated samples from the training set **with replacement**, and repreat this process B times to generate B different, independently generated bootstrapped training sets. We will discuss it more in the context of decision trees.\n",
    "\n",
    "To apply bagging to regression trees, we can construct B regression trees using B bootstrapped training sets mentioned above, and then average these predictions according to the equation:\n",
    "\\begin{gather*} \n",
    "f_{avg}(x) = \\frac{1}{B} \\sum_{b=1}^{B}f_b(x)    \n",
    "\\end{gather*}\n",
    "Notice we would use avg(f) for model aggregation if it is a regression and majority voting if it is a classification. Since each part of the ensemble could be done independently, we call it a **Parallel model training.**\n",
    "## Coding\n",
    "Next, Let's get hands dirty and have a practice to prove that Bagging has better performance than single learner with the following Wine classification problem. Let us suppose that we decide to use the features [ Alcohol, OD280/OD315 of diluted wines] to classify the Class label of wine after feature selection (we skip this part since it is not related to our topic today).\n",
    "\n",
    "**Data load and preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "VSIGe3RkDTqv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "'machine-learning-databases/wine/wine.data',header=None)\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "'Proline']\n",
    "\n",
    "df = df[df['Class label'] != 1]\n",
    "y = df['Class label'].values\n",
    "X = df[['Alcohol', 'OD280/OD315 of diluted wines']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.36</td>\n",
       "      <td>10.6</td>\n",
       "      <td>88</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.82</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>12.33</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.67</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>12.64</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.02</td>\n",
       "      <td>16.8</td>\n",
       "      <td>100</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.62</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.59</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>13.67</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>18.0</td>\n",
       "      <td>94</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.46</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2</td>\n",
       "      <td>12.37</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.16</td>\n",
       "      <td>19.0</td>\n",
       "      <td>87</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.87</td>\n",
       "      <td>4.45</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.87</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "59            2    12.37        0.94  1.36               10.6         88   \n",
       "60            2    12.33        1.10  2.28               16.0        101   \n",
       "61            2    12.64        1.36  2.02               16.8        100   \n",
       "62            2    13.67        1.25  1.92               18.0         94   \n",
       "63            2    12.37        1.13  2.16               19.0         87   \n",
       "\n",
       "    Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "59           1.98        0.57                  0.28             0.42   \n",
       "60           2.05        1.09                  0.63             0.41   \n",
       "61           2.02        1.41                  0.53             0.62   \n",
       "62           2.10        1.79                  0.32             0.73   \n",
       "63           3.50        3.10                  0.19             1.87   \n",
       "\n",
       "    Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "59             1.95  1.05                          1.82      520  \n",
       "60             3.27  1.25                          1.67      680  \n",
       "61             5.75  0.98                          1.59      450  \n",
       "62             3.80  1.23                          2.46      630  \n",
       "63             4.45  1.22                          2.87      420  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data and model setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lbencoder = LabelEncoder()\n",
    "y = lbencoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y,\n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=1,\n",
    "                                                   stratify=y)\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              max_depth=None,\n",
    "                              random_state=1)\n",
    "bc = BaggingClassifier(base_estimator=tree,\n",
    "                       n_estimators=300,\n",
    "                       max_samples=1.0,\n",
    "                       max_features=1.0,\n",
    "                       bootstrap=True,\n",
    "                       bootstrap_features=False,\n",
    "                       n_jobs=1,\n",
    "                       random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracies 0.8333\n",
      "Bagging test accuracies 0.9000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "bc = bc.fit(X_train, y_train)\n",
    "y_train_pred = bag.predict(X_train)\n",
    "y_test_pred = bag.predict(X_test)\n",
    "bag_train = accuracy_score(y_train, y_train_pred)\n",
    "bag_test = accuracy_score(y_test, y_test_pred)\n",
    "print('Decision tree test accuracies %.4f'% (tree_test))\n",
    "print('Bagging test accuracies %.4f'% (bag_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random_Forest\n",
    "While, sometimes, it is hard for bootstrapping to create many uncorrelated datasets. And highly correlated quantities do not lead to as large of a reduction in variance as compared to averaging many uncorrelated ones. So, to solve issue, we will introduce an extension of Bagging --- **Random Forest.**\n",
    "\n",
    "Random Forest (RF) [Breiman, 2001] is a representative of the state-of-art ensemble methods. It provides an improvement over bagged trees by way of a small tweak that achieves a decorrelation of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm_of_Random_Forest\n",
    "A random smaple of p features is selected **at each split** and the one out of only those p with the largest Gain is used to make the split. We draw a bootstrap dataset of size n from training data D. Then to build decision tree, we will recursively repeat the following splitting steps for each terminal/leaf node: \n",
    "1. Select p features from the original d features in D ramdomly. \n",
    "2. Pick the best feature among the p to split on.\n",
    "3. Split the node into descendant terminal nodes.\n",
    "\n",
    "The full logic is as following:\n",
    "\n",
    "\n",
    "**input**: Data set D = {(x1,y1),(x2,y2)....(xm,ym)}; Feature subset size K.\n",
    "\n",
    "**Process**:\n",
    "\n",
    "1.  create a tree node based on D --> N;\n",
    "2.  if all instances in the same class then return N;\n",
    "3.  the set of features that can be split further --> F;\n",
    "4.  if F is empty then return N;\n",
    "5.  select K features from F randomly --> F';\n",
    "6.  the feature which has the best split point in F' --> N.f;\n",
    "7.  the best split point on N.f --> N.p;\n",
    "8.  subset of D with values on N.f smaller than N.p --> Dl;\n",
    "9.  subset of D with values on N.f larger than or equal to N.p --> Dr;\n",
    "10.  call the process with parameters(Dl, K) --> Nl;\n",
    "11.  call the process with parameters(Dr, K) --> Nr;\n",
    "12.  return N\n",
    "\n",
    "**output**: A random decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boosting**\n",
    "Boosting is another ensemble method for improving the predictive performance. Instead of being a Parallel model training method, Boosting is **Sequential model training** process. The idea of boosting is to train many weak learners that learn to do well at different parts of the input space. You might ask what is different parts of input? The answer is: Parts at which trained weak learners make mistakes **previously**.\n",
    "\n",
    "## Regression\n",
    "Let us first talk about the Boosting in regression. Instead of fitting a single large decision tree, Boosting approach learns slowly iteratively (from previous steps).\n",
    "\n",
    "Given the current model from the previous step, we fit a regression model to the residuals from it as the response values, rather than the original response values. Then we add this new tree into the fitted function and update the residuals. We repeat this process for B times. The steps in details are as follows:\n",
    "1. Set f(x) = 0 and ri = yi for all instances i in training set D=(X,y)\n",
    "2. for b = 1,2,,,,B:\n",
    "    1. Fit a regression tree Tb to (X,r), regularized via a h_max or n_min (both are hyper-parameters). Denote the predictions of Tb on an instance x by fb(x).\n",
    "    2. Update prediction model f(x) + lambda * fb(Xi) --> f(x)\n",
    "    3. Update the residuals: ri - lambda * fb(xi) --> ri\n",
    "    \n",
    "    \n",
    "Output the boosted regression model \n",
    "\\begin{gather*} \n",
    "f(x) = \\sum_{b=1}^{B} \\lambda f_b(x)    \n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now let us discuss about how the idea of boosting could be applied to classification problems. Like we discussed a popular model of Bagging, we will also introduce you a super star of boosting --- **Adaboost** [Freund & Schapire, 1997].\n",
    "\n",
    "\n",
    "The idea is to repeatedly train classifier on modified versions of the data, thereby producing a sequence of weak classifiers f1, f2, ,,fB : x-->{-1,1}. The predictions from all these weak learners are combined through a **weighted** majority vote. At each iteration of the boosting, the **weight for a weak learner** is computed as a function that is inversely proportional to its training error(TE). On the other side, the **weight for a strong learner** is computed as a function that is proportional to its training error(TE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm_of_Adaboost\n",
    "**Input**: data set D={(x1,y1),(x2,y2)....(xm,ym)};\n",
    "    Base learning algorithm L;\n",
    "    Number of learning rounds T.\n",
    "**Process**:\n",
    "1. D1(x) = 1/m  Start with the same weight\n",
    "2. for t = 1,2,,,,T:\n",
    "    3. ht = L(D,Dt);    Train a classifier ht from D under distribution Dt\n",
    "    4. et = Px~dt(ht(x) != f(x));    Evaluate the error of ht\n",
    "    5. if et > 0.5 then break;\n",
    "    6. alpha(t) = 0.5 * ln((1 - et) / et)    Determine the weight of ht\n",
    "    7. D_{t+1}(x) = Dt(x) / Zt    Update the weight of learners, where Zt is a normalization factor.\n",
    "8. end\n",
    "**output**: \n",
    "\\begin{gather*} \n",
    "H(x) = sign(\\sum_{t=1}^{T} \\alpha_t h_t(x))   \n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Here, we add a practice to deepen your understanding of the algorithm behind Adaboost.\n",
    "\n",
    "**Question**: we will apply Adaboost to a toy dataset for classification. Consider the 2-d dataset in Figure 1. \n",
    "\n",
    "The dataset consists of the following 4 points: (x1, y1) : (<0,−1>, −), (x2, y2) :(<1, 0>, +), (x3, y3) : (<−1, 0>, +), (x4, y4) : (<0, 1>, −).Having decided to use simple decision stumps as weak classifiers, show how Adaboost works on this dataset for B = 4 iterations. For each  step, remember to compute quantities listed below, and fill in the corresponding entries in the table.\n",
    "\\begin{gather*} \n",
    "\\epsilon_b, \\alpha_b, \\beta_i^{b-1}, f_b^{(x_i)}  \n",
    "\\end{gather*}\n",
    "\n",
    "Note that at each iteration, there could be multiple decision stumps with same classification error. You can select the best learner at each iteration randomly. Draw your weak classifier that is learned on each iteration. For example, the selected learner f1 at b = 1 is shown in Figure 1 (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1**:\n",
    "<img src=\"1.png\" width=500 height=150>\n",
    "**table to be filled**:\n",
    "<img src=\"table.png\" width=500 height=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "Did you solve the problem and fill the table above? Here is the solution for you to check!\n",
    "<img src=\"solution.jpg\" width=500 height=150>\n",
    "<img src=\"table2.png\" width=500 height=150>\n",
    "\n",
    "\n",
    "Let us take the iteration 1 as example. At iteration 1, epsilon_b = 1/4 = 0.25. Then we can compute the alpha_b = ln(0.75/0.25)/2 = 0.549  \n",
    "\n",
    "Then we can compute beta_i according to this alpha_b. And don't forget to normalize them so that the sum of weights is equal to 1.\n",
    "\n",
    "Then move to iteration 2, there is still one error. And we could gain the new error rate by checking its weight gained from previous iteration.\n",
    "\n",
    "If you repeat this process, you should have gained the same answer as solution! Well Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              max_depth=1,\n",
    "                              random_state=1)\n",
    "ac = AdaBoostClassifier(base_estimator=tree,\n",
    "                         n_estimators=300,\n",
    "                         learning_rate=0.1,\n",
    "                         random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracies 0.867\n",
      "AdaBoost test accuracies 0.900\n"
     ]
    }
   ],
   "source": [
    "# Decision tree training\n",
    "tree = tree.fit(X_train, y_train)\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Adaboost training\n",
    "ac = ac.fit(X_train, y_train)\n",
    "y_train_pred = ada.predict(X_train)\n",
    "y_test_pred = ada.predict(X_test)\n",
    "ada_train = accuracy_score(y_train, y_train_pred)\n",
    "ada_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Results Printing\n",
    "print('Decision tree test accuracies %.3f'% (tree_test))\n",
    "print('AdaBoost test accuracies %.3f'% (ada_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging_vs_Boosting\n",
    "We will conclude this tutorial with a qualitative comparison of Bagging and Boosting methods.\n",
    "- Bagging creates training sets by bootstrapping, whereas Boosting(classification) reweights the trainning instances.\n",
    "- Bagging is a parallel model training, whereas Boosting is a sequential model training.\n",
    "- The weight of each learner is the same for Bagging, whereas weights in Boosting change based on their previous performances.\n",
    "- Bagging could reduce variance, whereas Boosting tend to reduce bias and variance at the same time.\n",
    "\n",
    "**You made it! Nice Job!** It is pleasure to be with you in this trip and talk about basic knowledge and algorithms behind ensemble learnings. I also provide my references for this tutorial and some other materials you might be interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Zhi-Hua Zhou, \"Ensemble Methods: Foundations and Algorithms\", CRC Press, 2012\n",
    "2. Trevor Hastie, Robert Tibshirani, \"An Introduction to Statistical Learning\" Chapter 8.2\n",
    "3. Trevor Hastie, Robert Tibshirani, Jerome Friedman, \"Elements of Statistical Learning\" Section 10.1\n",
    "4. Practice question: Upenn CIS520 Slides. https://canvas.upenn.edu/courses/1278549/files/55117366/download?verifier=bmvXrMQnOW9ji4DydSzxr7FnUNbbiwQzzRzbSl2Y&wrap=1\n",
    "5. Scikit Learn Ensemble Guide\n",
    "6. Kaggle Ensembling Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful_Resources\n",
    "1. Zhi-Hua Zhou, \"Ensemble Methods: Foundations and Algorithms\". https://tjzhifei.github.io/links/EMFA.pdf\n",
    "2. Trevor Hastie, Robert Tibshirani, \"An Introduction to Statistical Learning\". https://www.statlearning.com/\n",
    "3. Scikit Learn Ensemble Guide.  https://scikit-learn.org/stable/modules/ensemble.html\n",
    "4. Kaggle Ensembling Guide. https://mlwave.com/kaggle-ensembling-guide/\n",
    "5. Kaggle Winning Ensemble. https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\n",
    "6. github page about ensemble learning: https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb\n",
    "7. Ensemble Learning to Improve Machine Learning Results. https://blog.statsbot.co/ensemble-learning-d1dcd548e936"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "15-688 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
